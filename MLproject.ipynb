{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note : Import required packages and dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import nltk\n",
    "import time\n",
    "from nltk import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk import pos_tag, word_tokenize\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A. This project is a culmination of various aspects of AI.\n",
    "## 1. Machine Learning \n",
    "## 2. Deep Learning\n",
    "## 3. Knowledge Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B. Aim: The project can be classified under Natural Language Processing domain. The project will be aimed at trying to identify unwanted comments from the comment section of  wikipedia discussion pages. It is a multi-headed classification problem, where the comment wil get classified based on different types and each type will have levels. The user can then decide what levels of toxicsity are acceptible in the comment section.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C. The following steps will be followed during the course of the project. \n",
    "## 1. Importing the data\n",
    "## 2. Data Preprocessing\n",
    "###  a. Noise Removal\n",
    "###  b.Lexicon Normalization\n",
    "###  c.Lemmatization\n",
    "###  d.Stemming\n",
    "###  e.Object Standardization\n",
    "## 3. Text to Features (Feature Engineering on text data)\n",
    "###  a.Syntactical Parsing\n",
    "###  b.Dependency Grammar\n",
    "###  c.Part of Speech Tagging\n",
    "###  d.Entity Parsing\n",
    "###  e.Phrase Detection\n",
    "###  f. Named Entity Recognition\n",
    "###  g.Topic Modelling\n",
    "###  h. N-Grams\n",
    "###  i.  Statistical features\n",
    "###  j. TF â€“ IDF\n",
    "###  h. Frequency / Density Features\n",
    "###  j. Readability Features\n",
    "###  k.Word Embeddings\n",
    "## 4. Modelling \n",
    "###  a.Text Classification\n",
    "###  b.Text Matching\n",
    "###  c. Levenshtein Distance\n",
    "###  d. Phonetic Matching\n",
    "###  e.  Flexible String Matching\n",
    "###  f. Coreference Resolution\n",
    "## 5. Testing the accuracy of the Model\n",
    "###  a. Model on Test Data \n",
    "###  c. Human input testing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import the train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train.csv',encoding = 'utf8') as csvDataFile1:\n",
    "    train_data = pd.read_csv(csvDataFile1)\n",
    "with open('test.csv',encoding = 'utf8') as csvDataFile2:\n",
    "    test_data = pd.read_csv(csvDataFile2)\n",
    "with open('sample_submission.csv',encoding = 'utf8') as csvDataFile3:\n",
    "    sample_output = pd.read_csv(csvDataFile3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_output[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.a Noise Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Text to Features (Feature Engineering on text data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reads in both training and testing dataset\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = train.columns[2:].tolist()\n",
    "print(train.shape, test.shape)\n",
    "total_comments = train.shape[0] + test.shape[0]\n",
    "print(total_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df is the combination of training set comments and testing set comments\n",
    "merge = pd.concat([train.iloc[:,0:2],test.iloc[:,0:2]])\n",
    "df = merge.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should be performing Feature extraction and cleaning on the combined dataset of training instances and testing instances. Here the text features we are considering are as follows:\n",
    "1) Word Count 2) Unique Word Count 3) Sentence Count 4) Exclamation Mark Count 5) Capital Word Count 6) Percentage unique words in comment 7) Percentage capital words in comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count of words\n",
    "df['word_count']=df[\"comment_text\"].apply(lambda x: len(re.findall(pattern='[a-zA-Z]{2,25}',string=x)))\n",
    "# count of unique words\n",
    "df['unique_word_count']=df[\"comment_text\"].apply(lambda x: len(set([i.lower() for i in re.findall(pattern='[a-zA-Z]{2,25}',string=x)])))\n",
    "# count of sentences\n",
    "df['sentence_count']=df[\"comment_text\"].apply(lambda x: len(re.findall(pattern='\\n[^(\\n)|^( +\\n)]',string=x))+1)\n",
    "# count of exclamation marks\n",
    "df['exclamation_mark_count']=df[\"comment_text\"].apply(lambda x: len(re.findall(pattern='!',string=x))+1)\n",
    "# count of uppercase words\n",
    "df['capital_word_count']=df[\"comment_text\"].apply(lambda x: len(re.findall(pattern='[A-Z]{2,25}',string=x)))\n",
    "# percentage of unique words out of total words\n",
    "df['perc_unique_words'] = np.round(df['unique_word_count']/df['word_count'],2)\n",
    "# percentage of capital words out of total words\n",
    "df['perc_cap'] = np.round(df['capital_word_count']/df['word_count'],2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separates the training and testing features\n",
    "train_feats = df.iloc[0:len(train),]\n",
    "test_feats = df.iloc[len(train):,]\n",
    "print(train_feats.shape, test_feats.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the Tags for each comment with the features we calculated before\n",
    "train_tags = train.iloc[:,2:]\n",
    "train_feats=pd.concat([train_feats,train_tags],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corpus contains just the comments of the combined training and testing datasets \n",
    "corpus = merge.comment_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removal of everthing except words and also performs word lemmatization to have a completely cleaned corpus\n",
    "tokenizer = TweetTokenizer()\n",
    "def clean_corpus(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(\"\\n\",\" \",text)\n",
    "    text = re.sub(\"\\[.*\\]\",\" \",text)\n",
    "    text = re.sub(\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\",\" \",text)\n",
    "    text = re.sub(r\"\\?\",\" \",text)\n",
    "    text = re.sub(r'[^\\w\\s]','',text)\n",
    "    text = re.sub(\"\\d+\", \"\", text)\n",
    "    words = tokenizer.tokenize(text)\n",
    "    \n",
    "    # Performs Word Lemmatization\n",
    "    wnl = WordNetLemmatizer()\n",
    "    lemmed_words = []\n",
    "    for word, tag in pos_tag(words):\n",
    "        # NN: noun, common, singular or mass\n",
    "        if tag.startswith(\"NN\"):\n",
    "            lemmed_words.append(wnl.lemmatize(word, pos='n'))\n",
    "        # VB: verb, base form\n",
    "        elif tag.startswith('VB'):\n",
    "            lemmed_words.append(wnl.lemmatize(word, pos='v'))\n",
    "        # JJ: adjective or numeral, ordinal\n",
    "        elif tag.startswith('JJ'):\n",
    "            lemmed_words.append(wnl.lemmatize(word, pos='a'))\n",
    "        # R: adverb\n",
    "        elif tag.startswith('R'):\n",
    "            lemmed_words.append(wnl.lemmatize(word, pos='r'))    \n",
    "        else:\n",
    "            lemmed_words.append(word)  \n",
    "    \n",
    "    return(\" \".join(lemmed_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corpus.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_corpus(corpus.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_corpus = corpus.apply(lambda x : clean_corpus(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cleaned_corpus.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_unigrams=time.time()\n",
    "tfidf = TfidfVectorizer(min_df = 200,  max_features = 10000, strip_accents = 'unicode', \n",
    "                        analyzer = 'word', ngram_range = (1,1), use_idf = 1, smooth_idf = 1,\n",
    "                        sublinear_tf = True, stop_words = 'english')\n",
    "tfidf.fit(cleaned_corpus)\n",
    "features = np.array(tfidf.get_feature_names())\n",
    "\n",
    "end_unigrams=time.time()\n",
    "\n",
    "print(\"Total time to compute unigrams\",end_unigrams-start_unigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_unigrams =  tfidf.transform(cleaned_corpus.iloc[:train.shape[0]])\n",
    "test_unigrams = tfidf.transform(cleaned_corpus.iloc[train.shape[0]:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://buhrmann.github.io/tfidf-analysis.html\n",
    "def top_tfidf_feats(row, features, top_n=25):\n",
    "    ''' Get top n tfidf values in row and return them with their corresponding feature names.'''\n",
    "    topn_ids = np.argsort(row)[::-1][:top_n]\n",
    "    top_feats = [(features[i], row[i]) for i in topn_ids]\n",
    "    df = pd.DataFrame(top_feats)\n",
    "    df.columns = ['feature', 'tfidf']\n",
    "    return df\n",
    "\n",
    "def top_feats_in_doc(Xtr, features, row_id, top_n=25):\n",
    "    ''' Top tfidf features in specific document (matrix row) '''\n",
    "    row = np.squeeze(Xtr[row_id].toarray())\n",
    "    return top_tfidf_feats(row, features, top_n)\n",
    "\n",
    "def top_mean_feats(Xtr, features, grp_ids, min_tfidf=0.1, top_n=25):\n",
    "    ''' Return the top n features that on average are most important amongst documents in rows\n",
    "        indentified by indices in grp_ids. '''\n",
    "    \n",
    "    D = Xtr[grp_ids].toarray()\n",
    "\n",
    "    D[D < min_tfidf] = 0\n",
    "    tfidf_means = np.mean(D, axis=0)\n",
    "    return top_tfidf_feats(tfidf_means, features, top_n)\n",
    "\n",
    "# modified for multilabel milticlass\n",
    "def top_feats_by_class(Xtr, features, min_tfidf=0.1, top_n=20):\n",
    "    ''' Return a list of dfs, where each df holds top_n features and their mean tfidf value\n",
    "        calculated across documents with the same class label. '''\n",
    "    dfs = []\n",
    "    cols=train_tags.columns\n",
    "    for col in cols:\n",
    "        ids = train_tags.index[train_tags[col]==1]\n",
    "        feats_df = top_mean_feats(Xtr, features, ids, min_tfidf=min_tfidf, top_n=top_n)\n",
    "        feats_df.label = label\n",
    "        dfs.append(feats_df)\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get top n for unigrams\n",
    "tfidf_top_n_per_lass=top_feats_by_class(train_unigrams,features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Gensim\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "# Creating the term dictionary of our courpus, where every unique term is assigned an index. \n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "\n",
    "# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
